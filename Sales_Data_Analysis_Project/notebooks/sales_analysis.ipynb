{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e37f1cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ROLE 2: DATA ENGINEER - DATA CLEANING PROCESS\n",
      "============================================================\n",
      "\n",
      "\n",
      "1. IMPORTING DATASET...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Week-2-Sales-Data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m40\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Load the CSV file into a pandas DataFrame\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Note: Ensure the CSV file is in the same directory as this notebook\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mWeek-2-Sales-Data.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Display basic information about the dataset\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset imported successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Week-2-Sales-Data.csv'"
     ]
    }
   ],
   "source": [
    "# sales_analysis.ipynb\n",
    "\n",
    "\"\"\"\n",
    "SALES DATA ANALYSIS PROJECT\n",
    "================================\n",
    " Python Lead (Data Engineer)\n",
    " Python Analyst (KPI Calculator)\n",
    "\"\"\"\n",
    "\n",
    "# ============================================\n",
    "#  DATA ENGINEER - DATA CLEANING\n",
    "# ============================================\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ROLE 2: DATA ENGINEER - DATA CLEANING PROCESS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 1. IMPORT THE DATASET\n",
    "# -----------------------------------------------------------------\n",
    "print(\"1. IMPORTING DATASET...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "# Note: Ensure the CSV file is in the same directory as this notebook\n",
    "df = pd.read_csv('Week-2-Sales-Data.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset imported successfully!\")\n",
    "print(f\"Shape of dataset: {df.shape}\")  # (rows, columns)\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "print(f\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 2. INITIAL DATA EXPLORATION\n",
    "# -----------------------------------------------------------------\n",
    "print(\"2. INITIAL DATA EXPLORATION...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Display column names and data types\n",
    "print(\"Column Information:\")\n",
    "print(df.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display basic statistics for numerical columns\n",
    "print(\"Statistical Summary:\")\n",
    "print(df.describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display unique values in categorical columns\n",
    "print(\"Unique values in categorical columns:\")\n",
    "print(f\"Products: {df['Product'].unique()}\")\n",
    "print(f\"Regions: {df['Region'].unique()}\")\n",
    "print(f\"Sales Representatives: {df['Sales_Rep'].unique()}\")\n",
    "print(f\"Number of unique orders: {df['Order_ID'].nunique()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3. CHECK FOR DATA QUALITY ISSUES\n",
    "# -----------------------------------------------------------------\n",
    "print(\"3. CHECKING FOR DATA QUALITY ISSUES...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 3.1 Check for missing values\n",
    "print(\"3.1 Missing Values Check:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_report = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "print(missing_report[missing_report['Missing Values'] > 0])\n",
    "if missing_report[missing_report['Missing Values'] > 0].empty:\n",
    "    print(\"‚úì No missing values found!\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3.2 Check for duplicates\n",
    "print(\"3.2 Duplicates Check:\")\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "duplicate_order_ids = df['Order_ID'].duplicated().sum()\n",
    "print(f\"Total duplicate rows: {duplicate_rows}\")\n",
    "print(f\"Duplicate Order IDs: {duplicate_order_ids}\")\n",
    "if duplicate_rows == 0 and duplicate_order_ids == 0:\n",
    "    print(\"‚úì No duplicates found!\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3.3 Check data types\n",
    "print(\"3.3 Data Types Check:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3.4 Check for potential data inconsistencies\n",
    "print(\"3.4 Data Consistency Checks:\")\n",
    "print(\"Checking Revenue calculation consistency...\")\n",
    "# Calculate expected revenue from Units_Sold * Unit_Price\n",
    "df['Calculated_Revenue'] = df['Units_Sold'] * df['Unit_Price']\n",
    "\n",
    "# Compare with existing Revenue column\n",
    "revenue_discrepancies = df[df['Revenue'] != df['Calculated_Revenue']]\n",
    "print(f\"Revenue discrepancies found: {len(revenue_discrepancies)}\")\n",
    "if len(revenue_discrepancies) == 0:\n",
    "    print(\"‚úì All revenue calculations are consistent!\")\n",
    "else:\n",
    "    print(\"\\nRevenue discrepancies:\")\n",
    "    print(revenue_discrepancies[['Order_ID', 'Units_Sold', 'Unit_Price', 'Revenue', 'Calculated_Revenue']])\n",
    "print(\"\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 4. DATA CLEANING PROCESS\n",
    "# -----------------------------------------------------------------\n",
    "print(\"4. PERFORMING DATA CLEANING...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create a copy of the original dataframe for cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "# 4.1 Remove the temporary calculated column\n",
    "df_clean = df_clean.drop('Calculated_Revenue', axis=1)\n",
    "print(\"4.1 Removed temporary calculated column\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 4.2 Convert Order_Date to proper datetime format\n",
    "print(\"4.2 Converting Order_Date to datetime format...\")\n",
    "df_clean['Order_Date'] = pd.to_datetime(df_clean['Order_Date'])\n",
    "print(f\"‚úì Order_Date converted to datetime: {df_clean['Order_Date'].dtype}\")\n",
    "print(f\"Date range: {df_clean['Order_Date'].min()} to {df_clean['Order_Date'].max()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 4.3 Check for and handle any missing values (if any were found)\n",
    "# Since we found no missing values, this is just for demonstration\n",
    "print(\"4.3 Missing value handling (demonstration):\")\n",
    "if df_clean.isnull().sum().sum() > 0:\n",
    "    print(\"Handling missing values...\")\n",
    "    # For numerical columns, fill with median\n",
    "    # For categorical columns, fill with mode\n",
    "    numerical_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
    "    print(\"Missing values handled.\")\n",
    "else:\n",
    "    print(\"‚úì No missing values to handle!\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 4.4 Remove duplicates (if any were found)\n",
    "print(\"4.4 Duplicate handling (demonstration):\")\n",
    "initial_rows = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "final_rows = len(df_clean)\n",
    "duplicates_removed = initial_rows - final_rows\n",
    "\n",
    "if duplicates_removed > 0:\n",
    "    print(f\"Removed {duplicates_removed} duplicate rows\")\n",
    "else:\n",
    "    print(\"‚úì No duplicates to remove!\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 4.5 Validate data types\n",
    "print(\"4.5 Final data type validation:\")\n",
    "print(df_clean.dtypes)\n",
    "print(\"\\n\")\n",
    "\n",
    "# 4.6 Check for outliers in numerical columns\n",
    "print(\"4.6 Outlier detection (summary):\")\n",
    "numerical_cols = ['Units_Sold', 'Unit_Price', 'Revenue']\n",
    "for col in numerical_cols:\n",
    "    Q1 = df_clean[col].quantile(0.25)\n",
    "    Q3 = df_clean[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)]\n",
    "    print(f\"{col}: {len(outliers)} potential outliers detected\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 5. FINAL CLEANED DATASET SUMMARY\n",
    "# -----------------------------------------------------------------\n",
    "print(\"5. CLEANED DATASET SUMMARY\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Final shape: {df_clean.shape}\")\n",
    "print(f\"\\nFirst 3 rows of cleaned dataset:\")\n",
    "print(df_clean.head(3))\n",
    "print(f\"\\nData types after cleaning:\")\n",
    "print(df_clean.dtypes)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA CLEANING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60 + \"\\n\\n\")\n",
    "\n",
    "# ============================================\n",
    "# ROLE 3: PYTHON ANALYST - KPI CALCULATIONS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ROLE 3: PYTHON ANALYST - KPI CALCULATIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 1. SETUP FOR ANALYSIS\n",
    "# -----------------------------------------------------------------\n",
    "# We'll use the cleaned dataframe for all calculations\n",
    "df_analysis = df_clean.copy()\n",
    "\n",
    "# Add month and year columns for time-based analysis\n",
    "df_analysis['Order_Month'] = df_analysis['Order_Date'].dt.month\n",
    "df_analysis['Order_Year'] = df_analysis['Order_Date'].dt.year\n",
    "df_analysis['Month_Name'] = df_analysis['Order_Date'].dt.strftime('%B')\n",
    "\n",
    "print(\"Added time-based columns for analysis:\")\n",
    "print(df_analysis[['Order_Date', 'Order_Month', 'Month_Name', 'Order_Year']].head())\n",
    "print(\"\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 2. CALCULATE KEY PERFORMANCE INDICATORS (KPIs)\n",
    "# -----------------------------------------------------------------\n",
    "print(\"2. CALCULATING KEY PERFORMANCE INDICATORS\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\\n\")\n",
    "\n",
    "# KPI 1: Total revenue for the entire dataset\n",
    "print(\"KPI 1: TOTAL REVENUE\")\n",
    "print(\"-\" * 20)\n",
    "total_revenue = df_analysis['Revenue'].sum()\n",
    "print(f\"Total Revenue: R {total_revenue:,.2f}\")\n",
    "print(f\"Formatted: R {total_revenue:,.2f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# KPI 2: Average units sold per order\n",
    "print(\"KPI 2: AVERAGE UNITS SOLD PER ORDER\")\n",
    "print(\"-\" * 20)\n",
    "avg_units_sold = df_analysis['Units_Sold'].mean()\n",
    "print(f\"Average Units Sold per Order: {avg_units_sold:.2f}\")\n",
    "print(f\"Median Units Sold per Order: {df_analysis['Units_Sold'].median():.2f}\")\n",
    "print(f\"Range: {df_analysis['Units_Sold'].min()} to {df_analysis['Units_Sold'].max()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# KPI 3: Total revenue per region\n",
    "print(\"KPI 3: TOTAL REVENUE PER REGION\")\n",
    "print(\"-\" * 20)\n",
    "revenue_by_region = df_analysis.groupby('Region')['Revenue'].sum().sort_values(ascending=False)\n",
    "print(\"Revenue by Region (Descending Order):\")\n",
    "for region, revenue in revenue_by_region.items():\n",
    "    print(f\"  {region}: R {revenue:,.2f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Create a formatted table\n",
    "region_summary = pd.DataFrame({\n",
    "    'Region': revenue_by_region.index,\n",
    "    'Total Revenue': revenue_by_region.values,\n",
    "    'Percentage of Total': (revenue_by_region.values / total_revenue * 100).round(2)\n",
    "})\n",
    "print(region_summary.to_string(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "# KPI 4: Highest revenue-generating sales representative\n",
    "print(\"KPI 4: HIGHEST REVENUE-GENERATING SALES REPRESENTATIVE\")\n",
    "print(\"-\" * 20)\n",
    "revenue_by_rep = df_analysis.groupby('Sales_Rep')['Revenue'].sum().sort_values(ascending=False)\n",
    "print(\"Top 5 Sales Representatives by Revenue:\")\n",
    "for i, (rep, revenue) in enumerate(revenue_by_rep.head().items(), 1):\n",
    "    print(f\"  {i}. {rep}: R {revenue:,.2f}\")\n",
    "\n",
    "# Get the top rep\n",
    "top_rep = revenue_by_rep.index[0]\n",
    "top_rep_revenue = revenue_by_rep.iloc[0]\n",
    "print(f\"\\nüèÜ Highest Revenue Generating Sales Rep: {top_rep}\")\n",
    "print(f\"   Total Revenue Generated: R {top_rep_revenue:,.2f}\")\n",
    "print(f\"   Percentage of Total Revenue: {(top_rep_revenue/total_revenue*100):.2f}%\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# KPI 5: Top 3 products by total units sold\n",
    "print(\"KPI 5: TOP 3 PRODUCTS BY TOTAL UNITS SOLD\")\n",
    "print(\"-\" * 20)\n",
    "units_by_product = df_analysis.groupby('Product')['Units_Sold'].sum().sort_values(ascending=False)\n",
    "print(\"All Products by Units Sold:\")\n",
    "for product, units in units_by_product.items():\n",
    "    print(f\"  {product}: {units:,} units\")\n",
    "\n",
    "# Get top 3\n",
    "top_3_products = units_by_product.head(3)\n",
    "print(f\"\\nü•á Top 3 Products by Units Sold:\")\n",
    "for i, (product, units) in enumerate(top_3_products.items(), 1):\n",
    "    print(f\"  {i}. {product}: {units:,} units\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3. ADDITIONAL INSIGHTS AND ANALYSIS\n",
    "# -----------------------------------------------------------------\n",
    "print(\"3. ADDITIONAL INSIGHTS\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3.1 Monthly revenue trend\n",
    "print(\"3.1 MONTHLY REVENUE TREND\")\n",
    "monthly_revenue = df_analysis.groupby(['Order_Year', 'Order_Month', 'Month_Name'])['Revenue'].sum().reset_index()\n",
    "monthly_revenue = monthly_revenue.sort_values(['Order_Year', 'Order_Month'])\n",
    "\n",
    "# Create a readable month-year format\n",
    "monthly_revenue['Month_Year'] = monthly_revenue.apply(\n",
    "    lambda x: f\"{x['Month_Name']} {x['Order_Year']}\", axis=1\n",
    ")\n",
    "\n",
    "print(\"Monthly Revenue Summary:\")\n",
    "for _, row in monthly_revenue.iterrows():\n",
    "    print(f\"  {row['Month_Year']}: R {row['Revenue']:,.2f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3.2 Product performance by revenue\n",
    "print(\"3.2 PRODUCT PERFORMANCE BY REVENUE\")\n",
    "product_revenue = df_analysis.groupby('Product')['Revenue'].sum().sort_values(ascending=False)\n",
    "print(\"Products Ranked by Revenue:\")\n",
    "for i, (product, revenue) in enumerate(product_revenue.items(), 1):\n",
    "    print(f\"  {i}. {product}: R {revenue:,.2f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3.3 Regional performance analysis\n",
    "print(\"3.3 REGIONAL PERFORMANCE DEEP DIVE\")\n",
    "regional_stats = df_analysis.groupby('Region').agg({\n",
    "    'Revenue': ['sum', 'mean', 'count'],\n",
    "    'Units_Sold': ['sum', 'mean']\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "regional_stats.columns = ['_'.join(col).strip() for col in regional_stats.columns.values]\n",
    "print(regional_stats)\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3.4 Sales representative performance metrics\n",
    "print(\"3.4 SALES REPRESENTATIVE PERFORMANCE METRICS\")\n",
    "rep_performance = df_analysis.groupby('Sales_Rep').agg({\n",
    "    'Order_ID': 'count',\n",
    "    'Revenue': ['sum', 'mean'],\n",
    "    'Units_Sold': ['sum', 'mean']\n",
    "}).round(2)\n",
    "\n",
    "rep_performance.columns = ['Orders', 'Total_Revenue', 'Avg_Revenue', 'Total_Units', 'Avg_Units']\n",
    "rep_performance = rep_performance.sort_values('Total_Revenue', ascending=False)\n",
    "\n",
    "print(\"Top 10 Sales Representatives:\")\n",
    "print(rep_performance.head(10))\n",
    "print(\"\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 4. EXPORT RESULTS\n",
    "# -----------------------------------------------------------------\n",
    "print(\"4. EXPORTING RESULTS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create a summary dataframe for export\n",
    "summary_data = {\n",
    "    'KPI': [\n",
    "        'Total Revenue',\n",
    "        'Average Units per Order',\n",
    "        'Top Sales Representative',\n",
    "        'Top Sales Rep Revenue',\n",
    "        'Top Product by Units',\n",
    "        'Top Product Units Sold'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"R {total_revenue:,.2f}\",\n",
    "        f\"{avg_units_sold:.2f}\",\n",
    "        top_rep,\n",
    "        f\"R {top_rep_revenue:,.2f}\",\n",
    "        top_3_products.index[0],\n",
    "        f\"{top_3_products.iloc[0]:,}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Export to CSV for use in Excel report\n",
    "summary_df.to_csv('kpi_summary.csv', index=False)\n",
    "revenue_by_region.to_csv('revenue_by_region.csv')\n",
    "units_by_product.to_csv('units_by_product.csv')\n",
    "monthly_revenue.to_csv('monthly_revenue.csv')\n",
    "\n",
    "print(\"‚úì KPI Summary exported to 'kpi_summary.csv'\")\n",
    "print(\"‚úì Revenue by Region exported to 'revenue_by_region.csv'\")\n",
    "print(\"‚úì Units by Product exported to 'units_by_product.csv'\")\n",
    "print(\"‚úì Monthly Revenue exported to 'monthly_revenue.csv'\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 5. FINAL SUMMARY AND VISUALIZATION PREVIEW\n",
    "# -----------------------------------------------------------------\n",
    "print(\"=\" * 60)\n",
    "print(\"PROJECT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìä KEY FINDINGS:\")\n",
    "print(f\"1. Total Revenue: R {total_revenue:,.2f}\")\n",
    "print(f\"2. Average Units per Order: {avg_units_sold:.2f}\")\n",
    "print(f\"3. Top Region by Revenue: {revenue_by_region.index[0]} (R {revenue_by_region.iloc[0]:,.2f})\")\n",
    "print(f\"4. Top Sales Rep: {top_rep} (R {top_rep_revenue:,.2f})\")\n",
    "print(f\"5. Top Product by Units: {top_3_products.index[0]} ({top_3_products.iloc[0]:,} units)\")\n",
    "\n",
    "print(\"\\nüìà MONTHLY TREND:\")\n",
    "for _, row in monthly_revenue.iterrows():\n",
    "    print(f\"  {row['Month_Year']}: R {row['Revenue']:,.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display final dataframe info\n",
    "print(\"\\n\\nFINAL DATASET INFO:\")\n",
    "print(f\"Total Records: {len(df_analysis)}\")\n",
    "print(f\"Date Range: {df_analysis['Order_Date'].min().date()} to {df_analysis['Order_Date'].max().date()}\")\n",
    "print(f\"Unique Products: {df_analysis['Product'].nunique()}\")\n",
    "print(f\"Unique Regions: {df_analysis['Region'].nunique()}\")\n",
    "print(f\"Unique Sales Reps: {df_analysis['Sales_Rep'].nunique()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
